{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview: Adaptive RAG\n",
        "\n",
        "This project explores the design and implementation of an adaptive Retrieval-Augmented Generation (RAG) system enhanced with self-reflection capabilities. The goal is to reduce hallucinations in large language model (LLM) outputs by combining contextual retrieval with iterative reasoning.\n",
        "\n",
        "The system is built using LangChain, OpenAI language models, and Tavily Search API for web retrieval, with FAISS as the underlying vector store. It simulates an AI agent that can:\n",
        "\n",
        "- Retrieve relevant information from both preloaded documents and live web search\n",
        "\n",
        "- Generate answers using LLMs while referencing retrieved context\n",
        "\n",
        "- Evaluate its own response for relevance and confidence\n",
        "\n",
        "- Adaptively re-query or revise its answer if reflection indicates hallucination\n",
        "\n",
        "This prototype serves as a foundational experiment in building hallucination-aware agents, demonstrating how structured reasoning and adaptive behavior can be layered on top of RAG to make AI outputs more trustworthy and factually grounded."
      ],
      "metadata": {
        "id": "UM7EXqlrSVrw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zQPMWDXtGgQe"
      },
      "outputs": [],
      "source": [
        "# API keys\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_AI')\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Intalling libraries\n",
        "!pip install -q langchain_community langchain_openai faiss-cpu langgraph"
      ],
      "metadata": {
        "id": "EIQtXHLATblG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Vector Database\n",
        "\n",
        "In this step, we load documents from selected Medium articles and convert them into vector embeddings for efficient retrieval. The process involves:\n",
        "\n",
        "1. **Loading documents** from the given URLs using `WebBaseLoader`.\n",
        "2. **Generating embeddings** for the text using OpenAI's embedding model.\n",
        "3. **Storing embeddings** in a FAISS vector store to enable fast similarity search during retrieval.\n",
        "\n",
        "This forms the knowledge base that our Adaptive RAG agent will later use to ground its responses in reliable, retrievable context.\n"
      ],
      "metadata": {
        "id": "nljwXU6DQzD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Working on Retriever and Vector Store\n",
        "## Import Libraries\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "TL_Qp-fjGnOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Embeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "-SkDO3xPGnLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents to Index\n",
        "urls = [\n",
        "    \"https://medium.com/codex/what-are-ai-agents-your-step-by-step-guide-to-build-your-own-df54193e2de3\",  ## AI Agents\n",
        "    \"https://medium.com/@yaduvanshineelam09/introduction-to-fastapi-123c0b2778a5\",  ## Fast API\n",
        "    \"https://medium.com/@tuhinsharma121/understanding-adaptive-rag-smarter-faster-and-more-efficient-retrieval-augmented-generation-38490b6acf88\", ## Adaptive RAG\n",
        "    'https://medium.com/@kelseyywang/a-comprehensive-guide-to-llm-temperature-%EF%B8%8F-363a40bbc91f', ## LLM Temperature,\n",
        "    'https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f' ## LLM fundamentals\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "4K7Z_PY8GnJf"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]"
      ],
      "metadata": {
        "id": "WVmGs3a0GnHD"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_list[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "fHQvb_DdGnFE",
        "outputId": "8ab880a9-6ee2-43f1-c58c-046c7ebdda23"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What Are AI Agents? A Short Intro And A Step-by-Step Guide to Build Your Own. | by Maximilian Vogel | CodeX | MediumSitemapSign upSign inMedium LogoWriteSign upSign inCodeX·Everything connected with Tech & Code. Follow to join our 1M+ monthly readersWhat Are AI Agents? A Short Intro And A Step-by-Step Guide to Build Your Own.Maximilian VogelFollow8 min read·Dec 28, 2024--70ListenShareThe next big thing? Gartner believes AI agents are the future. OpenAI, Nvidia and Microsoft are betting on it — as are companies such as Salesforce, which have so far been rather inconspicuous in the field of AI.And there’s no doubt that the thing is really taking off right now.„AI Agents“ on Google Trends (trends.google.com)Wow.So, what is really behind the trend? The key to understanding agents is agency.Unlike traditional generative AI systems, agents don’t just respond to user input. Instead, they can process a complex problem such as an insurance claim from start to finish. This includes understanding the text, images and PDFs of the claim, retrieving information from the customer database, comparing the case with the insurance terms and conditions, asking the customer questions and waiting for their response — even if it takes days — without losing context.The agents do this autonomously — without humans having to check whether the AI is processing everything correctly.How I Got a Big AI Agent Up and Running — What Worked and What Didn’t.We went live in September last year with a logistics AI agent that autonomously retrieves lost shipments in…medium.comThe Espresso Machine and the BaristaIn contrast to existing AI systems and all the copilots out there that help employees to do their job, AI agents are, in fact, fully-fledged employees themselves, offering immense potential for process automation.Imagine — an AI that can take on complex, multi-step tasks that are currently performed by a human employee or an entire department:Planning, designing, executing, measuring, and optimizing a marketing campaignLocate a lost shipment in logistics by communicating with carriers, customers, and warehouses — or, if it remains lost, claim its value from the responsible partner.Search the trademark database each day and determine whether a new trademark has been registered that conflicts with my own trademark and immediately file an oppositiongather the relevant data or ask employees, check the data and compile an ESG reportCurrently, AI models can assist with tasks like generating campaign content or evaluating emails, but they lack the ability to execute an entire process. An AI agent can do that.Traditional generative AI can help human teams in a process (yellow), AI agents can execute the complete process end2end (orange). Image credit: Maximilian VogelWhile traditional models are like great espresso machines, agent-based AI is the barista. Not only can they make coffee, but they can welcome the guests, take the order, serve the coffee, collect the money, put the cups in the dishwasher, and even close up shop at night. Even the best espresso machine in the world can’t run a café by itself, but the barista can.Why can the AI agent and the barista do this? They excel at mastering various subprocesses of a complex job and can independently decide which task to tackle next. They can communicate with people, like the clients, if they need more information (milk or oat milk?). They can decide who they should ask in case of problems (beans are out => boss, coffee machine is on strike => customer service of the machine vendor).AI agents vs. traditional generative AI. Image credit: Maximilian VogelAnatomy of an AI WorkerBut enough chatting, let’s build an AI agent. Let us have a look at the relevant processes and workflows.Let us build an agent for the insurance process shown in the diagram above. The agent should handle an insurance claim from start to reimbursement.What we are developing here is the business architecture and the process flow. Unfortunately, I can’t dive into the coding because it can quickly become very extensive.1. Classification & sending a job into processing lanesOur workflow starts, when a customer sends a message with a claim for their home insurance to the insurer.What does our agent do? It determines what the customer wants by analyzing the message’s content.Based on this classification, the system initiates a processing lane. Often, this goes beyond function calling; it involves making a fundamental decision about the process, followed by executing many discrete steps.AI Agents: 1. Classify a mail and routing into different processing lanes. Image credit: Maximilian Vogel2. Extracting dataIn the next step, data is extracted. One of the main tasks of an agent is to turn unstructured data into structured data … to make processing systematic, safe and secure.Classification assigns a text to a predefined category, whereas extraction involves reading and interpreting data from the text. However, a language model doesn’t directly copy data from the input prompt; instead, it generates a response. This allows for data formatting, such as converting a phone number from ‘(718) 123–45678’ to ‘+1 718 123 45678’.AI Agents: 2. Extract data from the mail and attachments. Image credit: Maximilian VogelThe extraction of data is not limited to text content (from the e-mail text), but can also comprise data from images, PDFs or other documents. We use more than one model for that: LLMs, image recognition models, OCR and others. The above process is simplified, really massively simplified. In reality, we often send images to OCR systems that extract text from scanned invoices or forms.. And often we classify attachments as well, before analyzing them.We enforce JSON as the model’s output format to ensure structured data.This is the email input — unstructured data:Hi,I would like to report a damage and ask you to compensate me.Yesterday, while playing with a friend, my 9-year-old son Rajad kicked a soccer ball against the chandelier in the living room, which then broke from its holder and fell onto the floor and shattered (it was made of glass). Luckily no one is injured, but the chandelier is damaged beyond repair. Attached is an invoice and some images of the destroyed chandelier.Deepak Jamalcontract no: HC12-223873923123 Main Street10008 New York City(718) 123 45678This is the model output — a JSON, structured data:{  \"name\": \"Deepak\",  \"surname\": \"Jamal\",  \"address\": \"123 Main Street, 10008 New York City, NY\",  \"phone\":\"+1 718 123 45678\",  \"contract_no\": \"HC12-223873923\",  \"claim_description\": \"Yesterday [Dec-8, 2024], while playing with a friend, my 9-year-old son Rajad kicked a soccer ball against the chandelier in the living room, which then broke from its holder and fell onto the floor and shattered (it was made of glass).\\\\nLuckily no one is injured, but the chandelier is damaged beyond repair.\\\\n\"}3. Calling external services, making the context persistentMany generative AI systems can answer queries directly — sometimes using pre-trained data, fine-tuning, or Retrieval Augmented Generation (RAG) on some documents. This is not enough for agents. Almost every reasonably powerful AI agent needs to access corporate or external data from databases.To keep the context of a process persistent beyond the current session, it must also write data to systems and databases. In our case, the agent checks the contract number against a customer database and writes the status of the claim to an issue tracking system. It can also — remember: agency! — request missing data from external parties, such as the customer.AI Agents: 3. Call external services and make the context persistent. Image credit: Maximilian Vogel4. Assessment, RAG, reasoning and confidenceThe heart of every administration job consists of interpreting incoming cases in relation to various rules. AI is particularly good at this. Because we can’t provide all contextual information (e.g., policy content or terms and conditions) when calling a model, we use a vector database to retrieve relevant snippets — a technique known as RAG.And we prompt the AI to ‘think aloud’ before making an assessment. Thinking before blurting out the result improves answer quality — something we’ve all learned since 3rd grade math. We can also use the output of the model reasoning in many obvious and less obvious ways:- To substantiate an answer to the customer- To help the prompt engineer and data scientist figure out why the model made a mistake- For checks: Did the model arrive at the correct answer by chance, or can we see through its reasoning that the solution was inevitable?Here’s a little cheat sheet on reasoning and other prompt engineering techniques.Confidence is the key to maximizing accuracy. If the model estimates its confidence — and, dear prompt engineers, this also requires very good few shot learning examples for various confidence values — then we can configure the system to operate with extreme safety or high automation: We set a threshold of confidence below which all cases should go to human support. A high threshold ensures minimal errors but requires more manual processing, while a lower threshold allows more cases to be processed automatically, albeit with an increased risk of errors.AI Agents: 4. Use RAG / reasoning / confidence to obtain reliable assessments. Image credit: Maximilian VogelEt voila! If you have just implemented 2 or 3 of the above steps, you have developed an agent. I’ve outlined only the key components of these AI agents. You can certainly imagine the others. And you can either implement it with help of frameworks such as crewAI, langGraph, langFlow and their siblings or just do it in pure Python.Remarkably, such a system can automate 70%–90% of a claims management department’s workload. And that’s not possible with simple pre-agent generative AI systems. Two years ago, I could never have imagined this becoming reality so quickly.tl;dr? Here’s AI agents in a nutshell:The 3 Laws of AI Agents: Image credit: Maximilian Vogel with obvious borrowings from Isaac AsimovThese agents will certainly keep me busy over the coming months — my team and me have just launched a large logistics system.I wish you every success with your AI and agentic AI systems!And if you feel like it:Follow me on Medium (⇈) or LinkedIn for updates and new stories on generative AI, AI workers, and prompt engineering.Ai AgentGenerative AiMachine LearningReasoningAutomation----70FollowPublished in CodeX28K followers·Last published\\xa01 day agoEverything connected with Tech & Code. Follow to join our 1M+ monthly readersFollowFollowWritten by Maximilian Vogel13.3K followers·379 followingMachine learning, generative AI aficionado and speaker. Co-founder BIG PICTURE.FollowResponses (70)See all responsesHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Text\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 50)\n",
        "\n",
        "docs_split = text_splitter.split_documents(docs_list)"
      ],
      "metadata": {
        "id": "-A-ifaUZGnCx"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add to vectorstore\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=docs_split,\n",
        "    embedding=embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "FOFWXvyvGm-i"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Retriever\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "_0XtlYzLGm3K"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Analysis & Routing\n",
        "\n",
        "To improve the reliability and efficiency of our system, we introduce a lightweight **query classification module** that decides how each user query should be handled.\n",
        "\n",
        "- If the query is **factual or requires recent information**, it gets routed to a **web search retriever** using the Tavily API.\n",
        "- If the query is more **general, conceptual, or covered by our indexed documents**, it is routed to the **local FAISS vector store**.\n",
        "\n",
        "This step acts like a **router** that analyzes the query’s intent and dynamically selects the most appropriate source of knowledge.\n"
      ],
      "metadata": {
        "id": "ouPc4fg1ZMw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Router\n",
        "\n",
        "from typing import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field   # Validation\n",
        "\n",
        "# Data Model - Pydantic class for data validation\n",
        "class RouteQuery(BaseModel):\n",
        "  \"\"\"\n",
        "  Route the user query to the most relevant datasource\n",
        "  \"\"\"\n",
        "  datasource: Literal['vectorstore', 'web_search'] = Field(\n",
        "      ...,\n",
        "      description = \"Given a user question, decide whether to route it to web search or vectorstore\"\n",
        "  )"
      ],
      "metadata": {
        "id": "hNBu8HmdGmwG"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LLM with function call\n",
        "llm = ChatOpenAI(\n",
        "    model = 'gpt-4o-mini', temperature = 0\n",
        ")\n",
        "structured_llm_router = llm.with_structured_output(RouteQuery)    ## LLM will give the output whether websearhc or vectorstore"
      ],
      "metadata": {
        "id": "bAhf6u1DGmtf"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prompt\n",
        "system = \"\"\"\n",
        "You are an expert at routing a user question to a vectorstore or web search.\n",
        "The vectorstore contains documents related to AI agents, Adaptive RAG, LLM fundamentals, LLM Temperature and FastAPI.\n",
        "Use the vectorstore for questions on these topics. Else, use web-search\n",
        "\"\"\"\n",
        "\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [('system', system),\n",
        "     ('human', \"{question}\")]\n",
        ")\n",
        "\n",
        "question_router = route_prompt | structured_llm_router"
      ],
      "metadata": {
        "id": "f4IKR6D3ZQSk"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test question\n",
        "print(\n",
        "    question_router.invoke(\n",
        "        {\"question\": \"how temperature influences randomness of sampling in llms\"}\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sl_CD6BZQPF",
        "outputId": "da02c467-a4b8-49b5-8db7-812b90e03acb"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='vectorstore'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "This module evaluates the relevance of retrieved documents to the user query and filters out low-quality or off-topic results before answer generation.\n"
      ],
      "metadata": {
        "id": "QkOW61RMnxIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Model\n",
        "class GradeDocuments(BaseModel):\n",
        "  \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "  binary_score: str = Field(\n",
        "      description = \"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "  )"
      ],
      "metadata": {
        "id": "NlAFSjr-ZQM3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LLM With Function Call\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)"
      ],
      "metadata": {
        "id": "2yN12YoLZQK2"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "system = \"\"\"\n",
        "You are a grader assessing relevance of a retrieved document to a user question.\n",
        "If the document contains keywords or semantic meaning related to the user question, grade it as relevant.\n",
        "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
        "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\n",
        "\"\"\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [('system', system),\n",
        "     ('human', \"Retrieved document: \\n\\n{document} \\n\\n User question: {question}\")]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader"
      ],
      "metadata": {
        "id": "KlnPmq7LZQIQ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Adaptive RAG has query classification and self correction mechanism\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[0].page_content\n",
        "print(retrieval_grader.invoke({'question': question, 'document': doc_txt}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YgkhkgNZQEY",
        "outputId": "b74bbff6-ffab-4275-e142-019d54a655d9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Node\n",
        "\n",
        "This node uses the language model to generate an answer based on the top-ranked retrieved documents, ensuring the response is grounded in relevant context.\n"
      ],
      "metadata": {
        "id": "YhzxWgGFremX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate\n",
        "from langchain import hub ## We will pull predefined templates from here\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#Prompt\n",
        "prompt = hub.pull('rlm/rag-prompt')\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name = 'gpt-4o-mini', temperature = 0)\n",
        "\n",
        "# Post-processing - combining all retrieved text to be sent to RAG\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54TzC4uyZP_8",
        "outputId": "aacb4ec3-5a0d-44a7-edad-686e1b345274"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "generation = rag_chain.invoke({'context': format_docs(docs), 'question': question})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9efWuX-_ZP-E",
        "outputId": "b1d98c10-5cf9-4e7c-fda4-bc8f9060d564"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Adaptive-RAG includes a query classification mechanism that determines the complexity of a query and selects an appropriate retrieval strategy. It also features a self-correction mechanism that allows it to refine its responses based on the retrieved information. This combination enhances the system's efficiency and accuracy in answering queries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the prompt we used for RAG\n",
        "print(prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2128bjlZP7y",
        "outputId": "ac23a8fc-5747-4a75-9805-b2e2b4a47647"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: {question} \n",
            "Context: {context} \n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hallucination Check Node\n",
        "\n",
        "This node reviews the generated answer to identify potential hallucinations by checking factual consistency and alignment with the retrieved context.\n"
      ],
      "metadata": {
        "id": "qDr4Lb02t7fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Model\n",
        "class GradeHallucinations(BaseModel):\n",
        "  \"\"\"\n",
        "  Binary score for hallucination present in generation answer.\n",
        "  \"\"\"\n",
        "\n",
        "  binary_score: str = Field(\n",
        "      description=\"Answer is grounded in facts, 'yes' or 'no'\"\n",
        "  )\n"
      ],
      "metadata": {
        "id": "2WxoiRNWZP5P"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM with function call\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)"
      ],
      "metadata": {
        "id": "7d6q4tcoZP1U"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "system = \"\"\"\n",
        "You are a grader assessing whether an LLM generation is grounded in or supported by a set of retrieved facts.\n",
        "Give a binary score 'yes' or 'no'. 'Yes' means the answer is grounded in or supported by set of facts\n",
        "\"\"\"\n",
        "\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', system),\n",
        "        ('human', \"Set of facts: {documents} LLM generation: {generation}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader"
      ],
      "metadata": {
        "id": "EgsH9nEgZPzM"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hallucination_grader.invoke({'documents': format_docs(docs), 'generation': generation})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71r4S2msZPwy",
        "outputId": "5f8f5e50-1526-4d75-b3a9-7af179007c4c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer Grader\n",
        "\n",
        "This component assesses the final answer’s quality, fluency, and factual accuracy, helping determine if the response is ready to return or needs refinement.\n"
      ],
      "metadata": {
        "id": "Afs9xm9Zx-dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Model\n",
        "class GradeAnswer(BaseModel):\n",
        "  \"\"\"\n",
        "  Binary score to assess answer addresses qeustion.\n",
        "  \"\"\"\n",
        "\n",
        "  binary_score: str = Field(\n",
        "      description = \"Answer addresses the question, 'yes' or 'no'\"\n",
        "  )"
      ],
      "metadata": {
        "id": "awvBj0U3u1rO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM with function call\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)"
      ],
      "metadata": {
        "id": "UdS5XZU7u1o8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "system = \"\"\"\n",
        "You are a grader assessing whether an answer addresses and/or resolves a question.\n",
        "Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system',system),\n",
        "        ('human', 'User Question: {question}, LLM Answer: {generation}')\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader"
      ],
      "metadata": {
        "id": "tXagnQCtu1mt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_grader.invoke({'question': question, 'generation': generation})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TueJjjvu1kS",
        "outputId": "097214aa-553b-4b20-9504-45ca5e3451f2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question Re-Writer\n",
        "\n",
        "If the initial response is found lacking or hallucinated, this node rephrases the original question to improve retrieval and regenerate a more accurate answer.\n"
      ],
      "metadata": {
        "id": "AQATP65fzqYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "mQ2JvSkZ5hjk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser =  StrOutputParser()"
      ],
      "metadata": {
        "id": "wA89gmJi5rod"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0)"
      ],
      "metadata": {
        "id": "II3aPxbfzpDi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "system = \"\"\"\n",
        "You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval.\n",
        "Look at the input and try to reason about the underlying semantic intent and/or meaning.\n",
        "\"\"\"\n",
        "\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', system),\n",
        "        ('human', \"Here is the initial question: {question}. Formulate an improved question\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | parser"
      ],
      "metadata": {
        "id": "ljNWIU6Azo18"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_rewriter.invoke({'question': question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5J85sLeMzpp6",
        "outputId": "a27a9aa1-696b-4e66-941b-439871dd5c58"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are the query classification and self-correction mechanisms in Adaptive RAG?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating the WEB SEARCH tool"
      ],
      "metadata": {
        "id": "dIs6HISQ1b87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m2XDXGBzpnV",
        "outputId": "c2088c05-ba3a-4c15-e703-f1d2941c08af"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-1505316070>:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  web_search_tool = TavilySearchResults(k=3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph State\n",
        "\n",
        "This section defines the structure and flow of the LangGraph agent by connecting all the nodes—retrieval, generation, grading, and re-writing—into a coherent execution graph.\n"
      ],
      "metadata": {
        "id": "hufMwyji5FqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "  \"\"\"\n",
        "  Represents the state of our graph.\n",
        "\n",
        "  Attributes:\n",
        "    question: quesion\n",
        "    generation: LLM generation\n",
        "    documents: List of documents\n",
        "  \"\"\"\n",
        "\n",
        "  question: str\n",
        "  generation: str\n",
        "  documents: List[str]"
      ],
      "metadata": {
        "id": "oTuv2K8ZzpjS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining All Functionalities\n",
        "\n",
        "In this step, we bring together all the components—query router, retrievers, generator, hallucination checker, graders, and question re-writer—into a unified LangGraph workflow. This creates an adaptive, agentic RAG system capable of self-evaluating and improving its responses in real time.\n"
      ],
      "metadata": {
        "id": "KhzL97E_5vWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "\n",
        "    return {\"documents\": web_results, \"question\": question}\n",
        "\n",
        "\n",
        "### Edges ###\n",
        "\n",
        "\n",
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to web search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    if source.datasource == \"web_search\":\n",
        "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
        "        return \"web_search\"\n",
        "    elif source.datasource == \"vectorstore\":\n",
        "        print(\"---ROUTE QUESTION TO RAG---\")\n",
        "        return \"vectorstore\"\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\""
      ],
      "metadata": {
        "id": "5h04vxYLzphM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create State Graph\n",
        "\n",
        "This step builds the full LangGraph state machine by defining nodes and edges based on our adaptive RAG logic. The graph governs how queries flow through retrieval, generation, self-reflection, and correction steps.\n"
      ],
      "metadata": {
        "id": "cTCVhSkB6s8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, START, StateGraph"
      ],
      "metadata": {
        "id": "LOb7u9yqzpew"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(GraphState)"
      ],
      "metadata": {
        "id": "e7m9E52Rzomk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the nodes\n",
        "workflow.add_node(\"web_search\", web_search)  # web search\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shT346Xh7IPt",
        "outputId": "c11bab1d-6ff7-4bd9-a059-7b955fab2ebc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7a097b536710>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        'web_search':'web_search',\n",
        "        \"vectorstore\":\"retrieve\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge('web_search', 'generate')\n",
        "workflow.add_edge('retrieve', 'grade_documents')\n",
        "workflow.add_conditional_edges(\n",
        "    'grade_documents',\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        'transform_query':'transform_query',\n",
        "        'generate':'generate'\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge('transform_query','retrieve')\n",
        "workflow.add_conditional_edges(\n",
        "    'generate',\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        'not supported': 'generate',\n",
        "        'useful': END,\n",
        "        'not useful': 'transform_query'\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q72sXXQW7INo",
        "outputId": "820a5b5d-1ad9-415b-b4b7-71f2c524d799"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7a097b536710>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "mbe6dMbA7ILo"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({'question': \"Does Adaptive RAG reduce hallucinations?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7KJv-k37IGl",
        "outputId": "93a66ed1-9445-452b-901f-a9a2c4195a24"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO RAG---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['generation']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Dn4sB90rciaf",
        "outputId": "2eac0907-5e8c-4914-e228-17c0799a4d8f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Adaptive RAG minimizes hallucinations by intelligently determining the retrieval strategy based on the complexity of a query. It uses a classifier to assess whether a query is simple, moderate, or complex, allowing it to choose the appropriate level of document retrieval. This targeted approach ensures that the model relies on external information when necessary, reducing the likelihood of generating inaccurate responses.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking with a Traditional RAG"
      ],
      "metadata": {
        "id": "E6PhE7wqVrh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_vs_adaptive_rag_questions = [\n",
        "    # RAG-based questions\n",
        "    \"What are the three core components of an AI agent according to the Medium article by codex?\",\n",
        "    \"How does FastAPI improve the performance of API servers compared to Flask?\",\n",
        "    \"What role does the planner node play in an adaptive RAG system?\",\n",
        "    \"Explain the concept of temperature in LLMs and its effect on generation diversity.\",\n",
        "    \"What makes Adaptive RAG more efficient than Traditional RAG according to Tuhin Sharma’s article?\",\n",
        "    \"What is the purpose of tools like retriever and re-ranker in an adaptive RAG setup?\",\n",
        "    \"How do attention mechanisms in LLMs contribute to contextual understanding?\",\n",
        "    \"In the context of AI agents, what does it mean for an agent to be \\\"reactive\\\"?\",\n",
        "    \"What does Kelsey Wang say about the trade-off between temperature and coherence in generation?\",\n",
        "    \"According to the Microsoft article, how do transformers scale with data and model size?\",\n",
        "\n",
        "    # Web-search-based questions\n",
        "    \"What’s the latest open-source implementation of Adaptive RAG released in 2024?\",\n",
        "    \"Who are the major contributors to LangGraph's development in 2025?\",\n",
        "    \"What benchmark datasets are used to evaluate hallucination rates in RAG pipelines?\",\n",
        "    \"What is the most recent evaluation metric added to the Hugging Face leaderboard for factual grounding?\",\n",
        "    \"What are the latest best practices for setting LLM temperature when used in agentic systems?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "1DntsMIzVrTV"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_adaptive_rag = []\n",
        "for query in rag_vs_adaptive_rag_questions:\n",
        "  result = app.invoke({'question': query})\n",
        "  results_adaptive_rag.append(result['generation'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1uzCv_6OVrQ2",
        "outputId": "51ad0cb4-e330-424c-e55e-9ad91c6ab370"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO WEB SEARCH---\n",
            "---WEB SEARCH---\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "---RETRIEVE---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "GraphRecursionError",
          "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-72298274>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults_adaptive_rag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrag_vs_adaptive_rag_questions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mresults_adaptive_rag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2717\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2719\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   2720\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPH_RECURSION_LIMIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m                 )\n\u001b[0;32m-> 2456\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mGraphRecursionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m             \u001b[0;31m# set final channel values as run output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtViTvkvVrOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfeAyd6LVrL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsBBolTlVrJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPUe21vtVrG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkDcN1jZVrEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LK_LHHYUVrBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}